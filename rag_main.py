#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å –Ω–æ–≤–æ–π –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–π RAG —Å–∏—Å—Ç–µ–º—ã.
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –ø—Ä–æ–µ–∫—Ç–∞.
"""

import os
from dotenv import load_dotenv
from pathlib import Path

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# GPU detection and configuration
try:
    import torch
    GPU_AVAILABLE = torch.cuda.is_available()
    DEVICE = "cuda" if GPU_AVAILABLE else "cpu"
    print(f"üöÄ Device: {DEVICE}")
    if GPU_AVAILABLE:
        print(f"üì± GPU: {torch.cuda.get_device_name(0)}")
        print(f"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
except ImportError:
    GPU_AVAILABLE = False
    DEVICE = "cpu"
    print("‚ö†Ô∏è  PyTorch –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
from rag_base import RAGSystemFactory
from rag_python import PythonRAGSystem
from rag_universal import UniversalRAGSystem

# LangChain imports
from langchain_community.llms import YandexGPT
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.embeddings import HuggingFaceBgeEmbeddings


def load_model(model_name, local_dir="./models", wrapper_cls=HuggingFaceEmbeddings, use_gpu=None):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU.
    """
    if use_gpu is None:
        use_gpu = GPU_AVAILABLE
    
    local_path = Path(local_dir) / model_name.replace("/", "_")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è GPU
    model_kwargs = {}
    encode_kwargs = {}
    
    if use_gpu and GPU_AVAILABLE:
        model_kwargs.update({
            'device': DEVICE,
            'trust_remote_code': True,
        })
        encode_kwargs.update({
            'batch_size': 32,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º batch_size –¥–ª—è GPU
            'show_progress_bar': True,
        })
        print(f"üî• –ó–∞–≥—Ä—É–∂–∞–µ–º {model_name} –Ω–∞ GPU")
    else:
        model_kwargs.update({
            'device': 'cpu',
        })
        encode_kwargs.update({
            'batch_size': 8,  # –ú–µ–Ω—å—à–∏–π batch_size –¥–ª—è CPU
        })
        print(f"üêå –ó–∞–≥—Ä—É–∂–∞–µ–º {model_name} –Ω–∞ CPU")
    
    if local_path.exists():
        return wrapper_cls(
            model_name=str(local_path),
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )
    else:
        emb = wrapper_cls(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ
            if hasattr(emb, 'client'):
                emb.client.save(str(local_path))
            elif hasattr(emb, '_client'):
                emb._client.save(str(local_path))
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ: {e}")
        return emb


def create_rag_tool(
    project_path: str,
    rag_type: str = "auto",
    use_gpu: bool = None,
    **config
):
    """
    –°–æ–∑–¥–∞–µ—Ç RAG –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–µ–∫—Ç–∞.
    
    Args:
        project_path: –ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
        rag_type: –¢–∏–ø RAG —Å–∏—Å—Ç–µ–º—ã ("python", "universal", "auto")
        use_gpu: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU (None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        **config: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    """
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
    llm = YandexGPT(
        iam_token=os.getenv('YANDEX_GPT_API_KEY'),
        folder_id=os.getenv('YANDEX_GPT_FOLDER_ID'),
        model_name="yandexgpt"
    )
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø—Ä–æ–µ–∫—Ç–∞
    if rag_type == "auto":
        rag_type = RAGSystemFactory.detect_project_type(project_path)
        print(f"üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω —Ç–∏–ø –ø—Ä–æ–µ–∫—Ç–∞: {rag_type}")
    
    # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
    if rag_type == "python":
        # –î–ª—è Python –ø—Ä–æ–µ–∫—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        embeddings = load_model(
            model_name="BAAI/llm-embedder",
            wrapper_cls=HuggingFaceBgeEmbeddings,
            use_gpu=use_gpu
        )
    else:
        # –î–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–¥–æ–≤—É—é –º–æ–¥–µ–ª—å
        embeddings = load_model(
            model_name="BAAI/bge-code-v1",
            use_gpu=use_gpu
        )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º—ã
    print(f"üîß –°–æ–∑–¥–∞–µ–º {rag_type.upper()} RAG —Å–∏—Å—Ç–µ–º—É...")
    
    rag_system = RAGSystemFactory.create(
        rag_type,
        llm=llm,
        embeddings=embeddings,
        **config
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
    tool = rag_system.create_tool(project_path)
    
    print(f"‚úÖ {rag_type.upper()} RAG –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤!")
    return tool


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    project_path = os.getenv('TEST_PROJ_PATH')
    if not project_path:
        print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è TEST_PROJ_PATH")
        return
    
    print("ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ RAG –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤...")
    
    # –°–æ–∑–¥–∞–µ–º Python RAG –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
    print("\n" + "="*60)
    python_tool = create_rag_tool(
        project_path=project_path,
        rag_type="python",
        use_gpu=True
    )
    
    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π RAG –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
    print("\n" + "="*60)
    universal_tool = create_rag_tool(
        project_path=project_path,
        rag_type="universal",
        use_gpu=True
    )
    
    # –°–æ–∑–¥–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π RAG –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
    print("\n" + "="*60)
    auto_tool = create_rag_tool(
        project_path=project_path,
        rag_type="auto",
        use_gpu=True
    )
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    print("\n" + "="*60)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í")
    print("="*60)
    
    test_questions = [
        "–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–ª–∞—Å—Å PrefixedDBModel?",
        "–ö–∞–∫–∏–µ –µ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ?",
        "–ï—Å—Ç—å –ª–∏ Docker —Ñ–∞–π–ª—ã –∏ –∫–∞–∫ –æ–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã?",
    ]
    
    tools = {
        "Python RAG": python_tool,
        "Universal RAG": universal_tool,
        "Auto RAG": auto_tool
    }
    
    for question in test_questions:
        print(f"\n‚ùì –í–æ–ø—Ä–æ—Å: {question}")
        print("-" * 50)
        
        for tool_name, tool in tools.items():
            try:
                print(f"\nüîç {tool_name}:")
                result = tool.invoke(question)
                print(result[:200] + "..." if len(result) > 200 else result)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ {tool_name}: {e}")
        
        print("\n" + "="*60)


if __name__ == "__main__":
    main()
