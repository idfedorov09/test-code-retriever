#!/usr/bin/env python3
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è GPU –ø–∞–º—è—Ç—å—é –≤ RAG —Å–∏—Å—Ç–µ–º–µ
"""

import gc
import os
import psutil
from typing import Optional, Dict, Any
import warnings

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings("ignore")

class GPUMemoryManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä GPU –ø–∞–º—è—Ç–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–µ–∫"""
    
    def __init__(self):
        self.torch_available = False
        self.cuda_available = False
        self.device = None
        self._init_torch()
    
    def _init_torch(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PyTorch –∏ CUDA"""
        try:
            import torch
            self.torch_available = True
            self.cuda_available = torch.cuda.is_available()
            
            if self.cuda_available:
                self.device = torch.device('cuda')
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é
                os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
            else:
                self.device = torch.device('cpu')
                
        except ImportError:
            self.torch_available = False
            self.cuda_available = False
            self.device = None
    
    def get_gpu_memory_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ GPU –ø–∞–º—è—Ç–∏"""
        if not self.cuda_available:
            return {"gpu_available": False}
        
        try:
            import torch
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏
            total_memory = torch.cuda.get_device_properties(0).total_memory
            reserved_memory = torch.cuda.memory_reserved(0)
            allocated_memory = torch.cuda.memory_allocated(0)
            free_memory = total_memory - reserved_memory
            
            return {
                "gpu_available": True,
                "device_name": torch.cuda.get_device_name(0),
                "total_memory_gb": total_memory / (1024**3),
                "allocated_memory_gb": allocated_memory / (1024**3),
                "reserved_memory_gb": reserved_memory / (1024**3),
                "free_memory_gb": free_memory / (1024**3),
                "memory_usage_percent": (reserved_memory / total_memory) * 100
            }
        except Exception as e:
            return {"gpu_available": True, "error": str(e)}
    
    def cleanup_gpu_memory(self, aggressive: bool = False):
        """–û—á–∏—â–∞–µ—Ç GPU –ø–∞–º—è—Ç—å"""
        if not self.cuda_available:
            return
        
        try:
            import torch
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
            gc.collect()
            
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ PyTorch
            if hasattr(torch.cuda, 'empty_cache'):
                torch.cuda.empty_cache()
            
            if aggressive:
                # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ - —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏ –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
                torch.cuda.synchronize()
                gc.collect()
                torch.cuda.empty_cache()
                
                # –ü–æ–ø—ã—Ç–∫–∞ –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –ø–∞–º—è—Ç–∏
                if hasattr(torch.cuda, 'memory_snapshot'):
                    try:
                        torch.cuda.memory._dump_snapshot("memory_snapshot.pickle")
                        os.remove("memory_snapshot.pickle")  # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª —Å–Ω–∏–º–∫–∞
                    except:
                        pass
                
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ GPU –ø–∞–º—è—Ç–∏: {e}")
    
    def check_memory_threshold(self, threshold_percent: float = 85.0) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏ –ø–æ—Ä–æ–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        if not self.cuda_available:
            return False
        
        memory_info = self.get_gpu_memory_info()
        if "memory_usage_percent" in memory_info:
            return memory_info["memory_usage_percent"] > threshold_percent
        
        return False
    
    def get_optimal_batch_size(self, base_batch_size: int = 32) -> int:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        if not self.cuda_available:
            return base_batch_size
        
        memory_info = self.get_gpu_memory_info()
        if "free_memory_gb" not in memory_info:
            return base_batch_size
        
        free_memory_gb = memory_info["free_memory_gb"]
        
        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞
        if free_memory_gb > 10:
            return base_batch_size * 2
        elif free_memory_gb > 5:
            return base_batch_size
        elif free_memory_gb > 2:
            return max(base_batch_size // 2, 8)
        else:
            return max(base_batch_size // 4, 4)
    
    def monitor_memory_usage(self):
        """–í—ã–≤–æ–¥–∏—Ç —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
        memory_info = self.get_gpu_memory_info()
        
        if not memory_info.get("gpu_available", False):
            print("üêå GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
            return
        
        if "error" in memory_info:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU: {memory_info['error']}")
            return
        
        print(f"üî• GPU: {memory_info.get('device_name', 'Unknown')}")
        print(f"üìä –ü–∞–º—è—Ç—å: {memory_info['allocated_memory_gb']:.1f}GB / {memory_info['total_memory_gb']:.1f}GB "
              f"({memory_info['memory_usage_percent']:.1f}%)")
        print(f"üÜì –°–≤–æ–±–æ–¥–Ω–æ: {memory_info['free_memory_gb']:.1f}GB")
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        if memory_info['memory_usage_percent'] > 90:
            print("üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: –ü–∞–º—è—Ç—å GPU –ø–æ—á—Ç–∏ –∏—Å—á–µ—Ä–ø–∞–Ω–∞!")
        elif memory_info['memory_usage_percent'] > 75:
            print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞
gpu_manager = GPUMemoryManager()

def cleanup_gpu():
    """–ë—ã—Å—Ç—Ä–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏"""
    gpu_manager.cleanup_gpu_memory()

def aggressive_cleanup_gpu():
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏"""
    gpu_manager.cleanup_gpu_memory(aggressive=True)

def monitor_gpu():
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –ø–∞–º—è—Ç–∏"""
    gpu_manager.monitor_memory_usage()

def get_gpu_info():
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU"""
    return gpu_manager.get_gpu_memory_info()

def check_gpu_health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è GPU"""
    return not gpu_manager.check_memory_threshold()

if __name__ == "__main__":
    print("üîß GPU Memory Manager - –¢–µ—Å—Ç")
    print("=" * 50)
    
    monitor_gpu()
    
    print("\nüßπ –í—ã–ø–æ–ª–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É –ø–∞–º—è—Ç–∏...")
    cleanup_gpu()
    
    print("\nüìä –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏:")
    monitor_gpu()
