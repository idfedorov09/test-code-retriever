#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª—é–±—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –≤–º–µ—Å—Ç–æ —Å–ª–æ–∂–Ω–æ–≥–æ AST –ø–∞—Ä—Å–∏–Ω–≥–∞.
"""

from __future__ import annotations

import os
import re
import json
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS, DistanceStrategy
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

from rag_base import (
    BaseRAGSystem, BaseFileMap, BaseCodeElement, FileParser, DependencyAnalyzer,
    ProjectIndex, RAGSystemFactory, _relpath, _read_text
)


# -----------------------------------------------------------------------------
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
# -----------------------------------------------------------------------------

@dataclass
class UniversalCodeChunk(BaseCodeElement):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∫–æ–¥–∞"""
    content: str
    chunk_type: str  # "function", "class", "config", "generic"
    keywords: List[str] = field(default_factory=list)
    
    def to_line(self) -> str:
        return f"{self.chunk_type}: {self.name}  # L{self.lineno}-{self.end_lineno}"


@dataclass
class UniversalFileMap(BaseFileMap):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∫–∞—Ä—Ç–∞ —Ñ–∞–π–ª–∞"""
    chunks: List[UniversalCodeChunk]
    keywords: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    
    def to_text(self) -> str:
        parts = [f"FILE: {self.path} (type: {self.file_type})"]
        
        if self.keywords:
            parts.append("KEYWORDS: " + ", ".join(self.keywords[:10]))
        
        if self.dependencies:
            parts.append("DEPENDENCIES: " + ", ".join(self.dependencies[:5]))
        
        if self.chunks:
            parts.append("CHUNKS:")
            for chunk in self.chunks[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 —á–∞–Ω–∫–æ–≤
                parts.append(f"  - {chunk.to_line()}")
            if len(self.chunks) > 5:
                parts.append(f"  ... and {len(self.chunks) - 5} more chunks")
        
        parts.append(f"LOC: {self.loc}")
        return "\n".join(parts)
    
    def get_searchable_content(self) -> List[str]:
        content = []
        for chunk in self.chunks:
            content.append(f"{chunk.chunk_type}:{chunk.name}")
            content.extend([f"keyword:{kw}" for kw in chunk.keywords])
        return content


# -----------------------------------------------------------------------------
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
# -----------------------------------------------------------------------------

class UniversalFileParser(FileParser):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤
    FILE_TYPE_PATTERNS = {
        '.py': 'python',
        '.js': 'javascript', '.ts': 'typescript', '.jsx': 'react', '.tsx': 'react',
        '.html': 'html', '.css': 'css', '.scss': 'sass',
        '.sql': 'sql',
        '.yml': 'yaml', '.yaml': 'yaml',
        '.json': 'json',
        '.md': 'markdown', '.rst': 'rst',
        '.txt': 'text', '.log': 'log',
        '.env': 'env',
        '.toml': 'toml', '.ini': 'ini',
        '.sh': 'shell', '.bash': 'shell',
    }
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
    CONTEXT_PROMPTS = {
        'python': "Python –∫–æ–¥: —Ñ—É–Ω–∫—Ü–∏–∏, –∫–ª–∞—Å—Å—ã, –∏–º–ø–æ—Ä—Ç—ã, PEP8 —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã",
        'javascript': "JavaScript/TypeScript: —Ñ—É–Ω–∫—Ü–∏–∏, –∫–ª–∞—Å—Å—ã, –º–æ–¥—É–ª–∏, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏",
        'react': "React –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: JSX, —Ö—É–∫–∏, —Å–æ—Å—Ç–æ—è–Ω–∏–µ, props",
        'html': "HTML —Ä–∞–∑–º–µ—Ç–∫–∞: —Å–µ–º–∞–Ω—Ç–∏–∫–∞, –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å, SEO",
        'css': "CSS —Å—Ç–∏–ª–∏: —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, flexbox, grid, –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å",
        'sql': "SQL –∑–∞–ø—Ä–æ—Å—ã: —Ç–∞–±–ª–∏—Ü—ã, –∏–Ω–¥–µ–∫—Å—ã, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å",
        'yaml': "YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
        'json': "JSON –¥–∞–Ω–Ω—ã–µ: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –≤–∞–ª–∏–¥–∞—Ü–∏—è",
        'dockerfile': "Docker: –æ–±—Ä–∞–∑—ã, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞",
        'markdown': "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —Å—Å—ã–ª–∫–∏, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
        'shell': "Shell —Å–∫—Ä–∏–ø—Ç—ã: –∫–æ–º–∞–Ω–¥—ã, –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
        'env': "–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: —Å–µ–∫—Ä–µ—Ç—ã, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
    }
    
    def can_parse(self, file_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª —Ç–µ–∫—Å—Ç–æ–≤—ã–º"""
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ –Ω–µ–Ω—É–∂–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        path_parts = Path(file_path).parts
        skip_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'venv', 'env', '.env', 'dist', 'build'}
        
        if any(part in skip_dirs for part in path_parts):
            return False
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                f.read(1024)  # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ø–µ—Ä–≤—ã–µ 1KB
            return True
        except:
            return False
    
    def parse(self, file_path: str, root: str) -> Optional[UniversalFileMap]:
        try:
            content = _read_text(file_path)
            file_type = self._detect_file_type(file_path)
            
            chunks = self._split_into_chunks(content, file_type)
            keywords = self._extract_keywords(content, file_type)
            dependencies = self._extract_dependencies(content, file_type)
            
            return UniversalFileMap(
                path=_relpath(file_path, root),
                file_type=file_type,
                chunks=chunks,
                keywords=keywords,
                dependencies=dependencies,
                loc=len(content.splitlines())
            )
        except Exception:
            return None
    
    def _detect_file_type(self, path: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ–∞–π–ª–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        ext = Path(path).suffix.lower()
        filename = Path(path).name.lower()
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        if 'dockerfile' in filename or filename == 'dockerfile':
            return 'dockerfile'
        if 'docker-compose' in filename:
            return 'dockerfile'  # –¢–æ–∂–µ Docker
        if filename in {'makefile', 'cmake', 'cmakelists.txt', 'rakefile'}:
            return 'build'
        if filename in {'requirements.txt', 'package.json', 'composer.json', 'gemfile', 'cargo.toml', 'go.mod'}:
            return 'config'
        if filename.startswith('.env'):
            return 'env'
        if filename in {'readme.md', 'readme.rst', 'readme.txt', 'readme'}:
            return 'markdown'
        if filename.startswith('.') and not filename.endswith(('.py', '.js', '.ts')):
            return 'config'
        
        # –ü–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
        file_type = self.FILE_TYPE_PATTERNS.get(ext, 'text')
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        if file_type == 'text' and ext in {'.conf', '.cfg', '.ini', '.properties'}:
            return 'config'
        
        return file_type
    
    def _split_into_chunks(self, content: str, file_type: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–∞—Å—Ç–∏"""
        if file_type == 'python':
            return self._split_python_chunks(content)
        elif file_type in ['javascript', 'typescript', 'react']:
            return self._split_js_chunks(content)
        elif file_type == 'dockerfile':
            return self._split_dockerfile_chunks(content)
        elif file_type in ['yaml', 'json']:
            return self._split_config_chunks(content)
        elif file_type == 'sql':
            return self._split_sql_chunks(content)
        else:
            return self._split_generic_chunks(content)
    
    def _split_python_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ Python –∫–æ–¥–∞ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å—ã"""
        chunks = []
        lines = content.splitlines()
        current_chunk = []
        current_name = "module_level"
        current_type = "generic"
        start_line = 1
        
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            
            if stripped.startswith(('def ', 'class ', 'async def ')):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —á–∞–Ω–∫
                if current_chunk:
                    chunks.append(UniversalCodeChunk(
                        name=current_name,
                        content='\n'.join(current_chunk),
                        chunk_type=current_type,
                        lineno=start_line,
                        end_lineno=i-1,
                        keywords=self._extract_python_keywords('\n'.join(current_chunk))
                    ))
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫
                current_chunk = [line]
                start_line = i
                
                if stripped.startswith('class '):
                    current_type = "class"
                    current_name = stripped.split()[1].split('(')[0].rstrip(':')
                else:
                    current_type = "function"
                    current_name = stripped.split()[1].split('(')[0]
            else:
                current_chunk.append(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append(UniversalCodeChunk(
                name=current_name,
                content='\n'.join(current_chunk),
                chunk_type=current_type,
                lineno=start_line,
                end_lineno=len(lines),
                keywords=self._extract_python_keywords('\n'.join(current_chunk))
            ))
        
        return chunks
    
    def _split_js_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ JavaScript/React –∫–æ–¥–∞"""
        chunks = []
        lines = content.splitlines()
        current_chunk = []
        current_name = "module_level"
        current_type = "generic"
        start_line = 1
        
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            
            # –§—É–Ω–∫—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å—ã
            if (stripped.startswith(('function ', 'const ', 'let ', 'var ', 'class ', 'export function')) or
                'function' in stripped or '=>' in stripped):
                
                if current_chunk:
                    chunks.append(UniversalCodeChunk(
                        name=current_name,
                        content='\n'.join(current_chunk),
                        chunk_type=current_type,
                        lineno=start_line,
                        end_lineno=i-1,
                        keywords=self._extract_js_keywords('\n'.join(current_chunk))
                    ))
                
                current_chunk = [line]
                start_line = i
                
                if 'class ' in stripped:
                    current_type = "class"
                    current_name = re.search(r'class\s+(\w+)', stripped)
                    current_name = current_name.group(1) if current_name else "unknown_class"
                else:
                    current_type = "function"
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–º—è —Ñ—É–Ω–∫—Ü–∏–∏
                    match = re.search(r'(?:function\s+(\w+)|const\s+(\w+)|let\s+(\w+)|var\s+(\w+))', stripped)
                    current_name = next((g for g in match.groups() if g), "anonymous") if match else "anonymous"
            else:
                current_chunk.append(line)
        
        if current_chunk:
            chunks.append(UniversalCodeChunk(
                name=current_name,
                content='\n'.join(current_chunk),
                chunk_type=current_type,
                lineno=start_line,
                end_lineno=len(lines),
                keywords=self._extract_js_keywords('\n'.join(current_chunk))
            ))
        
        return chunks
    
    def _split_dockerfile_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ Dockerfile –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏"""
        chunks = []
        lines = content.splitlines()
        current_chunk = []
        current_name = "dockerfile_block"
        start_line = 1
        
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            
            if stripped.startswith(('FROM ', 'RUN ', 'COPY ', 'ADD ', 'WORKDIR ', 'EXPOSE ')):
                if current_chunk:
                    chunks.append(UniversalCodeChunk(
                        name=current_name,
                        content='\n'.join(current_chunk),
                        chunk_type="docker_command",
                        lineno=start_line,
                        end_lineno=i-1,
                        keywords=self._extract_docker_keywords('\n'.join(current_chunk))
                    ))
                
                current_chunk = [line]
                start_line = i
                current_name = stripped.split()[0].lower() + "_block"
            else:
                current_chunk.append(line)
        
        if current_chunk:
            chunks.append(UniversalCodeChunk(
                name=current_name,
                content='\n'.join(current_chunk),
                chunk_type="docker_command",
                lineno=start_line,
                end_lineno=len(lines),
                keywords=self._extract_docker_keywords('\n'.join(current_chunk))
            ))
        
        return chunks
    
    def _split_config_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        chunks = []
        
        try:
            import json
            data = json.loads(content)
            
            if isinstance(data, dict):
                # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–µ–∫—Ü–∏–π
                
                # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                main_info = {}
                for key in ['name', 'version', 'description', 'author', 'license']:
                    if key in data:
                        main_info[key] = data[key]
                
                if main_info:
                    chunks.append(UniversalCodeChunk(
                        name="main_info",
                        content=json.dumps(main_info, indent=2),
                        chunk_type="package_info",
                        lineno=1,
                        end_lineno=10,
                        keywords=list(main_info.keys())
                    ))
                
                # Dependencies
                if 'dependencies' in data:
                    deps = data['dependencies']
                    chunks.append(UniversalCodeChunk(
                        name="dependencies",
                        content=json.dumps(deps, indent=2),
                        chunk_type="dependencies",
                        lineno=1,
                        end_lineno=len(str(deps).splitlines()),
                        keywords=list(deps.keys())[:20]  # –ü–µ—Ä–≤—ã–µ 20 –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∫–∞–∫ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                    ))
                
                # Dev Dependencies
                if 'devDependencies' in data:
                    dev_deps = data['devDependencies']
                    chunks.append(UniversalCodeChunk(
                        name="devDependencies",
                        content=json.dumps(dev_deps, indent=2),
                        chunk_type="dev_dependencies",
                        lineno=1,
                        end_lineno=len(str(dev_deps).splitlines()),
                        keywords=list(dev_deps.keys())[:10]
                    ))
                
                # Scripts
                if 'scripts' in data:
                    scripts = data['scripts']
                    chunks.append(UniversalCodeChunk(
                        name="scripts",
                        content=json.dumps(scripts, indent=2),
                        chunk_type="scripts",
                        lineno=1,
                        end_lineno=len(str(scripts).splitlines()),
                        keywords=list(scripts.keys())
                    ))
                
                # –ï—Å–ª–∏ –Ω–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ–∫—Ü–∏–π, —Å–æ–∑–¥–∞–µ–º –æ–±—â–∏–π —á–∞–Ω–∫
                if not chunks:
                    chunks.append(UniversalCodeChunk(
                        name="config",
                        content=content,
                        chunk_type="configuration",
                        lineno=1,
                        end_lineno=len(content.splitlines()),
                        keywords=self._extract_config_keywords(content)
                    ))
            else:
                # –ù–µ —Å–ª–æ–≤–∞—Ä—å - —Å–æ–∑–¥–∞–µ–º –æ–±—ã—á–Ω—ã–π —á–∞–Ω–∫
                chunks.append(UniversalCodeChunk(
                    name="config",
                    content=content,
                    chunk_type="configuration",
                    lineno=1,
                    end_lineno=len(content.splitlines()),
                    keywords=self._extract_config_keywords(content)
                ))
                
        except (json.JSONDecodeError, Exception):
            # –ï—Å–ª–∏ –Ω–µ JSON –∏–ª–∏ –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ - —Å–æ–∑–¥–∞–µ–º –æ–±—ã—á–Ω—ã–π —á–∞–Ω–∫
            chunks.append(UniversalCodeChunk(
                name="config",
                content=content,
                chunk_type="configuration",
                lineno=1,
                end_lineno=len(content.splitlines()),
                keywords=self._extract_config_keywords(content)
            ))
        
        return chunks
    
    def _split_sql_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ SQL —Ñ–∞–π–ª–æ–≤"""
        chunks = []
        statements = re.split(r';\s*\n', content)
        
        for i, stmt in enumerate(statements):
            if stmt.strip():
                stmt_type = "query"
                stmt_name = f"statement_{i+1}"
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø SQL –≤—ã—Ä–∞–∂–µ–Ω–∏—è
                first_word = stmt.strip().split()[0].upper() if stmt.strip() else ""
                if first_word in ['CREATE', 'ALTER', 'DROP']:
                    stmt_type = "ddl"
                elif first_word in ['SELECT', 'INSERT', 'UPDATE', 'DELETE']:
                    stmt_type = "dml"
                
                chunks.append(UniversalCodeChunk(
                    name=stmt_name,
                    content=stmt,
                    chunk_type=stmt_type,
                    lineno=1,  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ
                    end_lineno=len(stmt.splitlines()),
                    keywords=self._extract_sql_keywords(stmt)
                ))
        
        return chunks
    
    def _split_generic_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã"""
        paragraphs = re.split(r'\n\s*\n', content)
        chunks = []
        
        for i, para in enumerate(paragraphs):
            if para.strip():
                chunks.append(UniversalCodeChunk(
                    name=f"section_{i+1}",
                    content=para,
                    chunk_type="text_section",
                    lineno=1,  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ
                    end_lineno=len(para.splitlines()),
                    keywords=self._extract_text_keywords(para)
                ))
        
        return chunks
    
    def _extract_keywords(self, content: str, file_type: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞"""
        if file_type == 'python':
            return self._extract_python_keywords(content)
        elif file_type in ['javascript', 'typescript', 'react']:
            return self._extract_js_keywords(content)
        elif file_type == 'dockerfile':
            return self._extract_docker_keywords(content)
        elif file_type in ['yaml', 'json']:
            return self._extract_config_keywords(content)
        elif file_type == 'sql':
            return self._extract_sql_keywords(content)
        else:
            return self._extract_text_keywords(content)
    
    def _extract_python_keywords(self, content: str) -> List[str]:
        keywords = []
        # –ò–º–ø–æ—Ä—Ç—ã
        keywords.extend(re.findall(r'import\s+(\w+)', content))
        keywords.extend(re.findall(r'from\s+(\w+)', content))
        # –§—É–Ω–∫—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å—ã
        keywords.extend(re.findall(r'def\s+(\w+)', content))
        keywords.extend(re.findall(r'class\s+(\w+)', content))
        return list(set(keywords))
    
    def _extract_js_keywords(self, content: str) -> List[str]:
        keywords = []
        # –ò–º–ø–æ—Ä—Ç—ã –∏ —ç–∫—Å–ø–æ—Ä—Ç—ã
        keywords.extend(re.findall(r'import.*from\s+[\'"]([^\'"]+)[\'"]', content))
        keywords.extend(re.findall(r'export\s+(?:function\s+)?(\w+)', content))
        # –§—É–Ω–∫—Ü–∏–∏
        keywords.extend(re.findall(r'function\s+(\w+)', content))
        keywords.extend(re.findall(r'const\s+(\w+)\s*=', content))
        return list(set(keywords))
    
    def _extract_docker_keywords(self, content: str) -> List[str]:
        keywords = []
        # Docker –∫–æ–º–∞–Ω–¥—ã
        keywords.extend(re.findall(r'FROM\s+([^\s]+)', content))
        keywords.extend(re.findall(r'COPY\s+([^\s]+)', content))
        keywords.extend(re.findall(r'RUN\s+(\w+)', content))
        return list(set(keywords))
    
    def _extract_config_keywords(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        keywords = []
        
        try:
            import json
            data = json.loads(content)
            
            if isinstance(data, dict):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–∏ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è
                keywords.extend(data.keys())
                
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è package.json/package-lock.json
                if 'dependencies' in data and isinstance(data['dependencies'], dict):
                    keywords.extend(list(data['dependencies'].keys())[:30])  # –ü–µ—Ä–≤—ã–µ 30 –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
                
                if 'devDependencies' in data and isinstance(data['devDependencies'], dict):
                    keywords.extend(list(data['devDependencies'].keys())[:15])  # –ü–µ—Ä–≤—ã–µ 15 dev –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
                
                if 'scripts' in data and isinstance(data['scripts'], dict):
                    keywords.extend(data['scripts'].keys())
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∞–∂–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                for key in ['name', 'version', 'description']:
                    if key in data and isinstance(data[key], str):
                        keywords.append(data[key])
            
        except (json.JSONDecodeError, Exception):
            # Fallback: –ø—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞
            keywords = re.findall(r'"(\w+)"\s*:', content)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        return list(set(keywords))[:50]
    
    def _extract_sql_keywords(self, content: str) -> List[str]:
        keywords = []
        # –¢–∞–±–ª–∏—Ü—ã
        keywords.extend(re.findall(r'FROM\s+(\w+)', content, re.IGNORECASE))
        keywords.extend(re.findall(r'JOIN\s+(\w+)', content, re.IGNORECASE))
        keywords.extend(re.findall(r'TABLE\s+(\w+)', content, re.IGNORECASE))
        return list(set(keywords))
    
    def _extract_text_keywords(self, content: str) -> List[str]:
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
        words = re.findall(r'\b[A-Za-z_]\w+\b', content)
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –æ–±—â–∏–µ —Å–ª–æ–≤–∞
        keywords = [w for w in words if len(w) > 3 and w.lower() not in {'this', 'that', 'with', 'from', 'have', 'will', 'been', 'were'}]
        return list(set(keywords[:20]))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def _extract_dependencies(self, content: str, file_type: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        deps = []
        
        if file_type == 'python':
            # Python –∏–º–ø–æ—Ä—Ç—ã
            deps.extend(re.findall(r'import\s+([^\s,]+)', content))
            deps.extend(re.findall(r'from\s+([^\s]+)\s+import', content))
        elif file_type in ['javascript', 'typescript']:
            # JS/TS –∏–º–ø–æ—Ä—Ç—ã
            deps.extend(re.findall(r'import.*from\s+[\'"]([^\'"]+)[\'"]', content))
            deps.extend(re.findall(r'require\([\'"]([^\'"]+)[\'"]\)', content))
        elif file_type == 'dockerfile':
            # Docker –æ–±—Ä–∞–∑—ã
            deps.extend(re.findall(r'FROM\s+([^\s]+)', content))
        
        return list(set(deps))


# -----------------------------------------------------------------------------
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞
# -----------------------------------------------------------------------------

class UniversalRAGSystem(BaseRAGSystem):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª—é–±—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self, llm, embeddings, **kwargs):
        parsers = [UniversalFileParser()]
        analyzers = []  # –ü–æ–∫–∞ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
        
        # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –ø–∞–º—è—Ç—å—é
        default_config = {
            'max_documents': 1000,          # –ú–∞–∫—Å–∏–º—É–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            'max_chunks_per_file': 30,      # –ú–∞–∫—Å–∏–º—É–º —á–∞–Ω–∫–æ–≤ –Ω–∞ —Ñ–∞–π–ª
            'max_file_size_kb': 1000,        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ KB
            'faiss_batch_size': 20,        # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è FAISS
            'bm25_k': 8,                   # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è BM25
            'dense_k': 8,                  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è dense retrieval
            'ensemble_weights': (0.6, 0.4), # BM25 –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ
            'map_char_budget': 20000,      # –ë—é–¥–∂–µ—Ç —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            'evidence_char_budget': 15000, # –ë—é–¥–∂–µ—Ç –¥–ª—è evidence
            'max_evidence_items': 6,       # –ú–∞–∫—Å–∏–º—É–º evidence items
            'answer_language': 'ru',
            'use_compression': False       # –û—Ç–∫–ª—é—á–∞–µ–º –∫–æ–º–ø—Ä–µ—Å—Å–∏—é –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ RAG
        }
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        merged_config = {**default_config, **kwargs}
        
        super().__init__(llm, embeddings, parsers, analyzers, **merged_config)
        
        print(f"‚öôÔ∏è  Universal RAG –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
        print(f"   üìÑ –ú–∞–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {self.config.get('max_documents')}")
        print(f"   üì¶ –†–∞–∑–º–µ—Ä FAISS –±–∞—Ç—á–∞: {self.config.get('faiss_batch_size')}")
        print(f"   üîç –ß–∞–Ω–∫–æ–≤ –Ω–∞ —Ñ–∞–π–ª: {self.config.get('max_chunks_per_file')}")
    
    def get_file_patterns(self) -> Dict[str, str]:
        return {"all": "**/*"}  # –í—Å–µ —Ñ–∞–π–ª—ã
    
    def get_evidence_prompt_template(self) -> ChatPromptTemplate:
        return ChatPromptTemplate.from_messages([
("system", """
You are a code reviewer analyzing a multi-language project. You will see a compact map of the repository with different file types and their contents.

The map shows:
- Files of different types (Python, JavaScript, Docker, YAML, etc.)
- Code chunks and their keywords
- Dependencies and relationships
- Configuration files and their settings

Given the user question, propose up to {max_items} precise evidence items (files/chunks to inspect) in JSON.
Each item must be an object with keys: file (relative path), symbol (chunk name or "*" for whole file), and reason.

IMPORTANT: 
- Paths must be repository-relative (e.g., `src/app.py`) ‚Äî NEVER absolute
- Copy the exact path shown after `FILE:` in the context
- Consider the file type when determining relevance
- For configuration questions, focus on config files (YAML, JSON, ENV)
- For architecture questions, look at multiple file types
- For questions about specific files (like "package.json", "Dockerfile"), always request the whole file with symbol "*"
- For questions about file statistics (lines, size), always request the whole file with symbol "*"
- For questions about dependencies in JSON files, request both the whole file and specific chunks like "dependencies"
- For questions about specific line ranges (e.g., "lines 100-200", "—Å—Ç—Ä–æ–∫–∏ 100-200"), use the line range as symbol (e.g., "100-200")
- For questions about specific line numbers (e.g., "line 11799-11833"), use the line range as symbol (e.g., "11799-11833")
- For questions about "what's in lines X-Y of file Z", use the line range as symbol (e.g., "11799-11833")
- Include both specific chunks and whole files for complete analysis

Respond ONLY with JSON array, no prose.
"""),
    ("human", "Question:\n{question}\n\nContext (map snippets):\n{context}\n")
])
    
    def get_answer_prompt_template(self) -> ChatPromptTemplate:
        return ChatPromptTemplate.from_messages([
    ("system", """
You are a senior software architect analyzing a multi-technology project for code review. Using the provided repository map and evidence, answer the question comprehensively.

The repository includes files of different types:
- Source code (Python, JavaScript, etc.)
- Configuration files (YAML, JSON, ENV)
- Infrastructure files (Dockerfile, docker-compose)
- Documentation (Markdown, README)
- Build files (Makefile, package.json)

For questions about:
- Architecture: Consider relationships between different components and technologies
- Configuration: Focus on settings, environment variables, and deployment setup
- Dependencies: Look at imports, package files, and external services
- Security: Check for hardcoded secrets, permissions, and best practices
- Performance: Analyze code patterns and configuration optimizations

Provide specific file:line references from the evidence and explain relationships clearly.
Be precise, cite concrete references as `file:line`, and provide actionable insights. 
Consider the multi-technology nature of the project in your analysis.
Reply in {answer_language}.
"""),
    ("human", "Question:\n{question}\n\nRepo map (summaries):\n{map_text}\n\nEvidence (code bodies):\n{evidence_text}\n")
])
    
    def _create_documents(self, file_maps: List[BaseFileMap]) -> List[Document]:
        docs = []
        universal_maps = [fm for fm in file_maps if isinstance(fm, UniversalFileMap)]
        
        MAX_DOCS = self.config.get('max_documents', 10000)  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
        MAX_CHUNKS_PER_FILE = self.config.get('max_chunks_per_file', 100)  # –£–≤–µ–ª–∏—á–µ–Ω–æ
        MAX_FILE_SIZE = self.config.get('max_file_size_kb', 1000)  # –£–≤–µ–ª–∏—á–µ–Ω–æ
        
        print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(universal_maps)} —Ñ–∞–π–ª–æ–≤")
        print(f"‚öôÔ∏è  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: –º–∞–∫—Å {MAX_DOCS} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {MAX_CHUNKS_PER_FILE} —á–∞–Ω–∫–æ–≤/—Ñ–∞–π–ª, –º–∞–∫—Å {MAX_FILE_SIZE}KB –Ω–∞ —Ñ–∞–π–ª")
        
        doc_count = 0
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ —Ä–∞–∑–º–µ—Ä—É
        universal_maps.sort(key=lambda fm: (self._get_file_priority(fm), fm.loc))
        
        # Create documents from file maps
        for fm in universal_maps:
            if doc_count >= MAX_DOCS:
                print(f"‚ö†Ô∏è  –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({MAX_DOCS}), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã")
                break
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã
            file_size_kb = fm.loc * 0.05  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
            if file_size_kb > MAX_FILE_SIZE:
                print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª: {fm.path} (~{file_size_kb:.1f}KB)")
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ–ª–µ–∑–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if not fm.chunks and fm.loc < 10:
                continue
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –≤—Å–µ–≥–æ —Ñ–∞–π–ª–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏—Ö –∏ –≤–∞–∂–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            if fm.loc < 100 and self._is_important_file(fm):  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 100 —Å—Ç—Ä–æ–∫
                file_content = fm.to_text()
                if len(file_content) < 3000:  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 3000 —Å–∏–º–≤–æ–ª–æ–≤
                    docs.append(Document(
                        page_content=file_content,
                        metadata={"source": fm.path, "type": f"{fm.file_type}-map", "loc": fm.loc}
                    ))
                    doc_count += 1
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤
            if fm.chunks:
                context_prompt = UniversalFileParser.CONTEXT_PROMPTS.get(fm.file_type, "")
                
                # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ —á–∞–Ω–∫–∏
                important_chunks = self._select_important_chunks(fm.chunks, MAX_CHUNKS_PER_FILE)
                
                for chunk in important_chunks:
                    if doc_count >= MAX_DOCS:
                        break
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–∞–Ω–∫–∞
                    chunk_content = chunk.content
                    if len(chunk_content) > 2000:  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 2000 —Å–∏–º–≤–æ–ª–æ–≤
                        chunk_content = chunk_content[:2000] + "...[truncated]"
                    
                    doc_content = f"""FILE: {fm.path} (type: {fm.file_type})
CONTEXT: {context_prompt}
CHUNK: {chunk.name} ({chunk.chunk_type})

{chunk_content}

KEYWORDS: {', '.join(chunk.keywords[:5])}"""  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                    
                    docs.append(Document(
                        page_content=doc_content,
                        metadata={
                            "source": fm.path,
                            "type": f"{fm.file_type}-chunk",
                            "chunk_name": chunk.name,
                            "chunk_type": chunk.chunk_type
                        }
                    ))
                    doc_count += 1
            
            # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ —Å–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            if fm.loc > 50:  # –§–∞–π–ª—ã –±–æ–ª—å—à–µ 50 —Å—Ç—Ä–æ–∫
                file_stats = f"""FILE: {fm.path} (type: {fm.file_type})
SIZE: {fm.loc} lines
KEYWORDS: {', '.join(fm.keywords[:10])}
DEPENDENCIES: {', '.join(fm.dependencies[:5])}

This is a large file with {fm.loc} lines. Use specific queries to get detailed information about its contents."""
                
                docs.append(Document(
                    page_content=file_stats,
                    metadata={
                        "source": fm.path,
                        "type": f"{fm.file_type}-stats",
                        "loc": fm.loc,
                        "is_large_file": True
                    }
                ))
                doc_count += 1
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π —Å–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤
        if universal_maps:
            file_types_summary = ["PROJECT OVERVIEW:"]
            type_counts = {}
            files_by_type = {}
            
            for fm in universal_maps:
                type_counts[fm.file_type] = type_counts.get(fm.file_type, 0) + 1
                if fm.file_type not in files_by_type:
                    files_by_type[fm.file_type] = []
                files_by_type[fm.file_type].append(fm.path)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–ø–æ–≤ –≤ —Å–≤–æ–¥–∫–µ
            for file_type, count in sorted(type_counts.items())[:15]:  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 15 —Ç–∏–ø–æ–≤
                context = UniversalFileParser.CONTEXT_PROMPTS.get(file_type, "")[:50]
                file_types_summary.append(f"  {file_type}: {count} files - {context}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Ç–∏–ø–æ–≤
                if file_type in ['dockerfile', 'yaml', 'json', 'python', 'javascript', 'typescript', 'react'] and count <= 10:
                    files_list = files_by_type[file_type][:8]  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 8 —Ñ–∞–π–ª–æ–≤
                    file_types_summary.append(f"    Files: {', '.join(files_list)}")
                    if len(files_by_type[file_type]) > 8:
                        file_types_summary.append(f"    ... and {len(files_by_type[file_type]) - 8} more")
            
            docs.append(Document(
                page_content="\n".join(file_types_summary),
                metadata={"source": "__overview__", "type": "project-summary"}
            ))
            doc_count += 1
            
            # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤ —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
            important_types = ['dockerfile', 'yaml', 'json', 'shell', 'config', 'env']
            for file_type in important_types:
                if file_type in files_by_type and len(files_by_type[file_type]) <= 12:
                    if doc_count >= MAX_DOCS - 5:  # –û—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –¥—Ä—É–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                        break
                        
                    type_summary = [f"{file_type.upper()} FILES IN PROJECT:"]
                    for fm in [f for f in universal_maps if f.file_type == file_type][:8]:  # –ú–∞–∫—Å–∏–º—É–º 8 —Ñ–∞–π–ª–æ–≤ —ç—Ç–æ–≥–æ —Ç–∏–ø–∞
                        type_summary.append(f"  FILE: {fm.path} ({fm.loc} lines)")
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
                        if fm.keywords:
                            key_words = fm.keywords[:5]
                            type_summary.append(f"    Keywords: {', '.join(key_words)}")
                        if fm.dependencies:
                            deps = fm.dependencies[:3]
                            type_summary.append(f"    Dependencies: {', '.join(deps)}")
                        
                        # –î–ª—è –æ—á–µ–Ω—å –≤–∞–∂–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                        if file_type in ['dockerfile', 'env'] and fm.chunks:
                            first_chunk = fm.chunks[0]
                            if len(first_chunk.content) < 500:  # –¢–æ–ª—å–∫–æ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ–∞–π–ª—ã
                                type_summary.append(f"    Content preview: {first_chunk.content[:200]}...")
                    
                    docs.append(Document(
                        page_content="\n".join(type_summary),
                        metadata={"source": f"__{file_type}_files__", "type": f"{file_type}-files-summary"}
                    ))
                    doc_count += 1
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {doc_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∏–∑ {len(universal_maps)} —Ñ–∞–π–ª–æ–≤)")
        return docs
    
    def _get_file_priority(self, fm: UniversalFileMap) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–∞–π–ª–∞ (–º–µ–Ω—å—à–µ = –≤–∞–∂–Ω–µ–µ)"""
        # –í–∞–∂–Ω—ã–µ —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤ –ø–æ–ª—É—á–∞—é—Ç –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        priority_map = {
            'python': 1,
            'javascript': 1, 'typescript': 1, 'react': 1,
            'dockerfile': 2,
            'yaml': 2, 'json': 2,
            'sql': 3,
            'markdown': 4,
            'text': 5
        }
        return priority_map.get(fm.file_type, 6)
    
    def _is_important_file(self, fm: UniversalFileMap) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –≤–∞–∂–Ω—ã–º –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        important_types = {'python', 'javascript', 'typescript', 'react', 'dockerfile', 'yaml', 'json'}
        important_names = {'readme', 'config', 'settings', 'requirements', 'package'}
        
        if fm.file_type in important_types:
            return True
        
        filename = fm.path.lower()
        return any(name in filename for name in important_names)
    
    def _select_important_chunks(self, chunks: List[UniversalCodeChunk], max_chunks: int) -> List[UniversalCodeChunk]:
        """–í—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        if len(chunks) <= max_chunks:
            return chunks
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —Ç–∏–ø—É —á–∞–Ω–∫–∞
        priority_order = {
            'class': 3,
            'function': 2,
            'configuration': 2,
            'docker_command': 1,
            'generic': 0,
            'text_section': 0
        }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ —Ä–∞–∑–º–µ—Ä—É
        sorted_chunks = sorted(
            chunks,
            key=lambda c: (
                priority_order.get(c.chunk_type, 0),
                len(c.keywords),  # –ë–æ–ª—å—à–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ = –≤–∞–∂–Ω–µ–µ
                len(c.content)    # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ = –≤–∞–∂–Ω–µ–µ
            ),
            reverse=True
        )
        
        return sorted_chunks[:max_chunks]
    
    def _build_search_index(
        self, 
        docs: List[Document], 
        project_path: str, 
        file_maps: List[BaseFileMap]
    ) -> ProjectIndex:
        print(f"üèóÔ∏è  –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º GPU —É—Ç–∏–ª–∏—Ç—ã –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        try:
            from gpu_utils import cleanup_gpu, monitor_gpu, gpu_manager
            GPU_UTILS_AVAILABLE = True
        except ImportError:
            GPU_UTILS_AVAILABLE = False
        
        # BM25 retriever (–±—ã—Å—Ç—Ä—ã–π, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU)
        print("üìù –°–æ–∑–¥–∞–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞...")
        bm25 = BM25Retriever.from_documents(docs)
        bm25.k = self.config.get('bm25_k', 12)
        
        # FAISS retriever —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        print("üîç –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º FAISS
        if GPU_UTILS_AVAILABLE:
            if gpu_manager.check_memory_threshold(70):
                print("‚ö†Ô∏è  –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏, –≤—ã–ø–æ–ª–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É...")
                cleanup_gpu()
            monitor_gpu()
        
        try:
            # –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            batch_size = self.config.get('faiss_batch_size', 50)  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä
            
            if len(docs) > batch_size:
                print(f"üì¶ –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ {batch_size}")
                
                # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å –ø–æ —á–∞—Å—Ç—è–º
                first_batch = docs[:batch_size]
                faiss = FAISS.from_documents(
                    first_batch, 
                    self.embeddings, 
                    distance_strategy=DistanceStrategy.COSINE
                )
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –±–∞—Ç—á–∞–º–∏
                remaining_docs = docs[batch_size:]
                for i in range(0, len(remaining_docs), batch_size):
                    batch = remaining_docs[i:i + batch_size]
                    batch_num = i // batch_size + 2
                    total_batches = (len(docs) - 1) // batch_size + 1
                    print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_num}/{total_batches} ({len(batch)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
                    
                    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                    if GPU_UTILS_AVAILABLE:
                        cleanup_gpu()
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –∏–Ω–¥–µ–∫—Å—É
                    batch_texts = [doc.page_content for doc in batch]
                    batch_metadatas = [doc.metadata for doc in batch]
                    faiss.add_texts(batch_texts, batch_metadatas)
            else:
                # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤
                print(f"üìÑ –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                faiss = FAISS.from_documents(
                    docs, 
                    self.embeddings, 
                    distance_strategy=DistanceStrategy.COSINE
                )
            
            print("‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞: {e}")
            if GPU_UTILS_AVAILABLE:
                cleanup_gpu()
            
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ BM25
            print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ BM25-only —Ä–µ–∂–∏–º")
            return ProjectIndex(
                root=project_path,
                docs=docs,
                retriever=bm25,
                file_maps=file_maps
            )
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞
        if GPU_UTILS_AVAILABLE:
            cleanup_gpu()
            print("üìä –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏:")
            monitor_gpu()
        
        dense_retriever = faiss.as_retriever(search_kwargs={"k": self.config.get('dense_k', 12)})
        
        # Ensemble retriever
        print("ü§ù –°–æ–∑–¥–∞–Ω–∏–µ ensemble retriever...")
        ensemble = EnsembleRetriever(
            retrievers=[bm25, dense_retriever], 
            weights=list(self.config.get('ensemble_weights', (0.5, 0.5)))
        )
        
        print("‚úÖ –ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤!")
        return ProjectIndex(
            root=project_path,
            docs=docs,
            retriever=ensemble,
            file_maps=file_maps
        )
    
    def _answer_question(self, question: str, index: ProjectIndex) -> str:
        # Retrieve relevant documents
        evidence_char_budget = self.config.get('evidence_char_budget', 15000)  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ RAG
        max_evidence_items = self.config.get('max_evidence_items', 6)  # –£–º–µ–Ω—å—à–µ–Ω–æ
        map_char_budget = self.config.get('map_char_budget', 20000)  # –£–º–µ–Ω—å—à–µ–Ω–æ

        retrieved = index.retriever.invoke(question)
        
        # Gather context
        map_text = self._gather_map_snippets(retrieved, map_char_budget)
        
        # Generate evidence plan
        evidence_prompt = self.get_evidence_prompt_template()
        evidence_chain = evidence_prompt | self.llm | StrOutputParser()

        raw_plan = evidence_chain.invoke({
            "question": question,
            "context": map_text,
            "max_items": max_evidence_items,
        })
        
        plan_json = raw_plan.strip()
        plan_json = plan_json[plan_json.find("[") : plan_json.rfind("]") + 1] if "[" in plan_json and "]" in plan_json else "[]"
        try:
            plan = json.loads(plan_json)
            if not isinstance(plan, list):
                plan = []
        except Exception:
            plan = []

        # Fetch requested bodies
        evidence_pairs = self._extract_bodies(index.root, plan[:max_evidence_items]) if plan else []
        evidence_text = "\n\n".join([f"### {lbl}\n" + code for (lbl, code) in evidence_pairs])
        evidence_text = self._trim_to_chars(evidence_text, evidence_char_budget)
        
        # Generate final answer
        answer_prompt = self.get_answer_prompt_template()
        answer_chain = answer_prompt | self.llm | StrOutputParser()
        
        final = answer_chain.invoke({
            "question": question,
            "map_text": map_text,
            "answer_language": self.config.get('answer_language', 'ru'),
            "evidence_text": evidence_text if evidence_text else "(no additional bodies requested)",
        })
        
        return final
    
    def _extract_bodies(self, root: str, requests: List[Dict[str, str]]) -> List[Tuple[str, str]]:
        """Return list of (label, code_block) for requested {file,symbol}."""
        out: List[Tuple[str, str]] = []
        
        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        available_files = {}
        for root_dir, dirs, files in os.walk(root):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            dirs[:] = [d for d in dirs if d not in {'.git', 'node_modules', '__pycache__', '.venv', 'venv', 'dist', 'build'}]
            for file in files:
                file_path = os.path.join(root_dir, file)
                rel_path = os.path.relpath(file_path, root)
                available_files[file.lower()] = rel_path
                available_files[rel_path.lower()] = rel_path
        
        for req in requests:
            rel = req.get("file", "").strip()
            sym = req.get("symbol", "").strip()
            
            if not rel:
                continue
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
            target_path = None
            
            # 1. –ü—Ä—è–º–æ–π –ø—É—Ç—å
            direct_path = os.path.join(root, rel)
            if os.path.isfile(direct_path):
                target_path = direct_path
            
            # 2. –ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (case-insensitive)
            elif rel.lower() in available_files:
                target_path = os.path.join(root, available_files[rel.lower()])
            
            # 3. –ü–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
            else:
                rel_lower = rel.lower()
                for filename, filepath in available_files.items():
                    if (rel_lower in filename or 
                        filename in rel_lower or
                        rel_lower in filepath.lower() or
                        filepath.lower().endswith(rel_lower)):
                        target_path = os.path.join(root, filepath)
                        break
            
            if not target_path or not os.path.isfile(target_path):
                continue
            
            try:
                src = _read_text(target_path)
                rel_for_output = os.path.relpath(target_path, root)
                file_size = len(src)
                line_count = len(src.splitlines())
            except Exception:
                continue

            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤
            if sym == "*" or not sym:
                # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É
                if file_size > 10000:  # –ë–æ–ª—å—à–µ 10KB
                    content = self._handle_large_file(src, rel_for_output, file_size, line_count)
                else:
                    content = src
                out.append((f"{rel_for_output}:1", content))
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –ª–∏ –¥–∏–∞–ø–∞–∑–æ–Ω —Å—Ç—Ä–æ–∫
                line_range = self._extract_line_range_from_symbol(sym)
                if line_range:
                    start_line, end_line = line_range
                    if start_line <= line_count and end_line <= line_count:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã–µ —Å—Ç—Ä–æ–∫–∏
                        lines = src.splitlines()
                        requested_lines = lines[start_line-1:end_line]
                        
                        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–æ–∫
                        result = f"# –°—Ç—Ä–æ–∫–∏ {start_line}-{end_line} –∏–∑ {rel_for_output}\n\n"
                        result += "```\n"
                        for i, line in enumerate(requested_lines, start_line):
                            result += f"{i:4d}: {line}\n"
                        result += "```\n"
                        
                        out.append((f"{rel_for_output}:{start_line}-{end_line}", result))
                    else:
                        # –°—Ç—Ä–æ–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
                        out.append((f"{rel_for_output}:{start_line}-{end_line}", 
                                  f"# –°—Ç—Ä–æ–∫–∏ {start_line}-{end_line} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ñ–∞–π–ª–µ {rel_for_output}\n"
                                  f"–§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç {line_count} —Å—Ç—Ä–æ–∫"))
                else:
                    # –ò—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —á–∞–Ω–∫ –∏–ª–∏ —Å–∏–º–≤–æ–ª
                    found = self._find_symbol_in_file(src, sym, rel_for_output)
                    if found:
                        out.append(found)
                    else:
                        # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —á—Ç–æ —Å–∏–º–≤–æ–ª –Ω–µ –Ω–∞–π–¥–µ–Ω
                        content = f"# Symbol '{sym}' not found in file, showing full content:\n\n"
                        content += src[:3000] + "...[truncated]" if len(src) > 3000 else src
                        out.append((f"{rel_for_output}:1", content))

        return out
    
    def _handle_large_file(self, content: str, file_path: str, file_size: int, line_count: int) -> str:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã —Å —É–º–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        file_ext = file_path.split('.')[-1].lower()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∞–π–ª–∞
        stats = f"# –§–∞–π–ª: {file_path}\n"
        stats += f"# –†–∞–∑–º–µ—Ä: {file_size:,} –±–∞–π—Ç\n"
        stats += f"# –°—Ç—Ä–æ–∫: {line_count:,}\n\n"
        
        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤
        result = stats
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª–∞
        lines = content.splitlines()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞
        result += "## üìÑ –ù–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞\n\n"
        result += "```\n"
        for i, line in enumerate(lines[:20], 1):
            result += f"{i:4d}: {line}\n"
        result += "```\n\n"
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        if file_ext == 'json':
            result += self._analyze_json_structure(content)
        elif file_ext in ['py', 'js', 'ts', 'jsx', 'tsx']:
            result += self._analyze_code_structure(content, file_ext)
        elif file_ext in ['md', 'txt', 'rst']:
            result += self._analyze_text_structure(content)
        elif file_ext in ['yaml', 'yml']:
            result += self._analyze_yaml_structure(content)
        elif file_ext in ['xml', 'html']:
            result += self._analyze_xml_structure(content)
        else:
            result += self._analyze_generic_structure(content)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞
        if line_count > 40:
            result += "## üìÑ –ö–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞\n\n"
            result += "```\n"
            for i, line in enumerate(lines[-10:], line_count - 9):
                result += f"{i:4d}: {line}\n"
            result += "```\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π)
        result += "## üìÑ –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–ø–µ—Ä–≤—ã–µ 3000 —Å–∏–º–≤–æ–ª–æ–≤)\n\n"
        result += "```\n"
        result += content[:3000]
        if len(content) > 3000:
            result += "\n... [truncated]"
        result += "\n```\n"
        
        return result
    
    def _analyze_json_structure(self, content: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É JSON —Ñ–∞–π–ª–∞"""
        try:
            import json
            data = json.loads(content)
            
            result = "## üìä JSON –°—Ç—Ä—É–∫—Ç—É—Ä–∞\n\n"
            
            if isinstance(data, dict):
                result += f"**–ö–æ—Ä–Ω–µ–≤—ã–µ –∫–ª—é—á–∏:** {', '.join(data.keys())}\n\n"
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
                if 'dependencies' in data:
                    deps = data['dependencies']
                    result += f"**Dependencies:** {len(deps)} –ø–∞–∫–µ—Ç–æ–≤\n"
                    for name, version in list(deps.items())[:10]:
                        result += f"- {name}: {version}\n"
                    if len(deps) > 10:
                        result += f"... –∏ –µ—â–µ {len(deps) - 10} –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n"
                    result += "\n"
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥—Ä—É–≥–∏–µ –≤–∞–∂–Ω—ã–µ –∫–ª—é—á–∏
                for key in ['name', 'version', 'description', 'scripts', 'engines']:
                    if key in data:
                        result += f"**{key.title()}:** {data[key]}\n"
            else:
                result += f"**–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö:** {type(data).__name__}\n"
            
            return result
            
        except json.JSONDecodeError:
            return "## ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON\n\n"
        except Exception as e:
            return f"## ‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ JSON: {e}\n\n"
    
    def _analyze_code_structure(self, content: str, file_ext: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª–∞ —Å –∫–æ–¥–æ–º"""
        lines = content.splitlines()
        
        result = f"## üìù –ê–Ω–∞–ª–∏–∑ {file_ext.upper()} —Ñ–∞–π–ª–∞\n\n"
        
        # –°—á–∏—Ç–∞–µ–º —Ñ—É–Ω–∫—Ü–∏–∏, –∫–ª–∞—Å—Å—ã, –∏–º–ø–æ—Ä—Ç—ã
        functions = []
        classes = []
        imports = []
        
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            if stripped.startswith(('def ', 'function ', 'const ', 'let ', 'var ')):
                functions.append(f"–°—Ç—Ä–æ–∫–∞ {i}: {stripped}")
            elif stripped.startswith(('class ', 'interface ')):
                classes.append(f"–°—Ç—Ä–æ–∫–∞ {i}: {stripped}")
            elif stripped.startswith(('import ', 'from ', 'require(')):
                imports.append(f"–°—Ç—Ä–æ–∫–∞ {i}: {stripped}")
        
        if functions:
            result += f"**–§—É–Ω–∫—Ü–∏–∏ ({len(functions)}):**\n"
            for func in functions[:10]:
                result += f"- {func}\n"
            if len(functions) > 10:
                result += f"... –∏ –µ—â–µ {len(functions) - 10} —Ñ—É–Ω–∫—Ü–∏–π\n"
            result += "\n"
        
        if classes:
            result += f"**–ö–ª–∞—Å—Å—ã ({len(classes)}):**\n"
            for cls in classes[:10]:
                result += f"- {cls}\n"
            if len(classes) > 10:
                result += f"... –∏ –µ—â–µ {len(classes) - 10} –∫–ª–∞—Å—Å–æ–≤\n"
            result += "\n"
        
        if imports:
            result += f"**–ò–º–ø–æ—Ä—Ç—ã ({len(imports)}):**\n"
            for imp in imports[:10]:
                result += f"- {imp}\n"
            if len(imports) > 10:
                result += f"... –∏ –µ—â–µ {len(imports) - 10} –∏–º–ø–æ—Ä—Ç–æ–≤\n"
            result += "\n"
        
        return result
    
    def _analyze_text_structure(self, content: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        lines = content.splitlines()
        
        result = "## üìÑ –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞\n\n"
        
        # –°—á–∏—Ç–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å—Å—ã–ª–∫–∏, —Å–ø–∏—Å–∫–∏
        headers = []
        links = []
        lists = []
        
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            if stripped.startswith('#'):
                headers.append(f"–°—Ç—Ä–æ–∫–∞ {i}: {stripped}")
            elif 'http' in stripped or 'www.' in stripped:
                links.append(f"–°—Ç—Ä–æ–∫–∞ {i}: {stripped}")
            elif stripped.startswith(('-', '*', '+', '1.', '2.', '3.')):
                lists.append(f"–°—Ç—Ä–æ–∫–∞ {i}: {stripped}")
        
        if headers:
            result += f"**–ó–∞–≥–æ–ª–æ–≤–∫–∏ ({len(headers)}):**\n"
            for header in headers[:10]:
                result += f"- {header}\n"
            result += "\n"
        
        if links:
            result += f"**–°—Å—ã–ª–∫–∏ ({len(links)}):**\n"
            for link in links[:5]:
                result += f"- {link}\n"
            result += "\n"
        
        if lists:
            result += f"**–°–ø–∏—Å–∫–∏ ({len(lists)}):**\n"
            for item in lists[:5]:
                result += f"- {item}\n"
            result += "\n"
        
        return result
    
    def _analyze_yaml_structure(self, content: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É YAML —Ñ–∞–π–ª–∞"""
        lines = content.splitlines()
        
        result = "## üìÑ –ê–Ω–∞–ª–∏–∑ YAML —Ñ–∞–π–ª–∞\n\n"
        
        # –°—á–∏—Ç–∞–µ–º —Å–µ–∫—Ü–∏–∏ (–∫–ª—é—á–∏ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è)
        sections = []
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            if stripped and not stripped.startswith('#') and not stripped.startswith('-') and ':' in stripped:
                key = stripped.split(':')[0].strip()
                if not key.startswith(' '):  # –¢–æ–ª—å–∫–æ –∫–ª—é—á–∏ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è
                    sections.append(f"–°—Ç—Ä–æ–∫–∞ {i}: {key}")
        
        if sections:
            result += f"**–°–µ–∫—Ü–∏–∏ ({len(sections)}):**\n"
            for section in sections[:10]:
                result += f"- {section}\n"
            result += "\n"
        
        return result
    
    def _analyze_xml_structure(self, content: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É XML/HTML —Ñ–∞–π–ª–∞"""
        lines = content.splitlines()
        
        result = "## üìÑ –ê–Ω–∞–ª–∏–∑ XML/HTML —Ñ–∞–π–ª–∞\n\n"
        
        # –°—á–∏—Ç–∞–µ–º —Ç–µ–≥–∏
        tags = []
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            if '<' in stripped and '>' in stripped:
                # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤
                import re
                found_tags = re.findall(r'<(\w+)', stripped)
                for tag in found_tags:
                    tags.append(f"–°—Ç—Ä–æ–∫–∞ {i}: <{tag}>")
        
        if tags:
            result += f"**–¢–µ–≥–∏ ({len(tags)}):**\n"
            for tag in tags[:10]:
                result += f"- {tag}\n"
            result += "\n"
        
        return result
    
    def _analyze_generic_structure(self, content: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ–±—ã—á–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        lines = content.splitlines()
        
        result = "## üìÑ –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞\n\n"
        
        # –°—á–∏—Ç–∞–µ–º –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        non_empty = [i for i, line in enumerate(lines, 1) if line.strip()]
        
        result += f"**–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**\n"
        result += f"- –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(lines)}\n"
        result += f"- –ù–µ–ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫: {len(non_empty)}\n"
        result += f"- –ü—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫: {len(lines) - len(non_empty)}\n\n"
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        if non_empty:
            result += f"**–ü–µ—Ä–≤—ã–µ –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏:**\n"
            for i in non_empty[:10]:
                result += f"–°—Ç—Ä–æ–∫–∞ {i}: {lines[i-1].strip()}\n"
            result += "\n"
        
        return result
    
    def _extract_line_range_from_symbol(self, symbol: str) -> Optional[Tuple[int, int]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω —Å—Ç—Ä–æ–∫ –∏–∑ —Å–∏–º–≤–æ–ª–∞"""
        import re
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "11799-11833" –∏–ª–∏ "11799:11833"
        patterns = [
            r'(\d+)-(\d+)',  # 11799-11833
            r'(\d+):(\d+)',  # 11799:11833
            r'—Å—Ç—Ä–æ–∫[–∏–∞]?\s+(\d+)-(\d+)',  # —Å—Ç—Ä–æ–∫–∏ 11799-11833
            r'—Å—Ç—Ä–æ–∫[–∏–∞]?\s+(\d+):(\d+)',  # —Å—Ç—Ä–æ–∫–∏ 11799:11833
            r'line[s]?\s+(\d+)-(\d+)',  # lines 11799-11833
            r'line[s]?\s+(\d+):(\d+)',  # lines 11799:11833
        ]
        
        for pattern in patterns:
            match = re.search(pattern, symbol, re.IGNORECASE)
            if match:
                start_line = int(match.group(1))
                end_line = int(match.group(2))
                return (start_line, end_line)
        
        return None
    
    def _find_symbol_in_file(self, content: str, symbol: str, file_path: str) -> Optional[Tuple[str, str]]:
        """–ò—â–µ—Ç —Å–∏–º–≤–æ–ª –≤ —Ñ–∞–π–ª–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç"""
        lines = content.splitlines()
        
        # –ò—â–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ñ—É–Ω–∫—Ü–∏–∏/–∫–ª–∞—Å—Å–∞/—Å–µ–∫—Ü–∏–∏
        for i, line in enumerate(lines):
            if (symbol in line and 
                any(keyword in line.lower() for keyword in ['def ', 'class ', 'function', 'const ', 'let ', 'var '])):
                # –ù–∞–π–¥–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º, –∏–∑–≤–ª–µ–∫–∞–µ–º –±–ª–æ–∫
                start_line = i + 1
                
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–æ–Ω–µ—Ü –±–ª–æ–∫–∞ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
                end_line = min(i + 50, len(lines))  # –ú–∞–∫—Å–∏–º—É–º 50 —Å—Ç—Ä–æ–∫
                for j in range(i + 1, len(lines)):
                    if lines[j].strip() and not lines[j].startswith((' ', '\t')):
                        end_line = j
                        break
                
                block_content = '\n'.join(lines[i:end_line])
                return (f"{file_path}:{start_line}", block_content)
        
        return None
    
    def _gather_map_snippets(self, docs: List[Document], max_chars: int = 20000) -> str:
        pieces = []
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        priority_types = [
            "project-summary",
            "dockerfile-files-summary", "yaml-files-summary", "json-files-summary", 
            "shell-files-summary", "config-files-summary", "env-files-summary",
            "python-map", "javascript-map", "typescript-map", "react-map",
            "dockerfile-map", "yaml-map", "json-map", "text-map", "markdown-map"
        ]
        
        # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for priority_type in priority_types:
            for d in docs:
                if d.metadata.get("type") == priority_type:
                    pieces.append(f"---\n{d.page_content}\n")
                    if sum(len(p) for p in pieces) > max_chars:
                        break
            if sum(len(p) for p in pieces) > max_chars:
                break
        
        # –ó–∞—Ç–µ–º –¥–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Å—Ç–æ
        if sum(len(p) for p in pieces) < max_chars * 0.8:  # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –º–µ–Ω—å—à–µ 80%
            for d in docs:
                doc_type = d.metadata.get("type", "")
                if doc_type.endswith("-chunk") and doc_type not in [p.split("\n")[1] for p in pieces]:
                    pieces.append(f"---\n{d.page_content}\n")
                    if sum(len(p) for p in pieces) > max_chars:
                        break
        
        return self._trim_to_chars("\n".join(pieces), max_chars)
    
    def _trim_to_chars(self, text: str, limit: int) -> str:
        if len(text) <= limit:
            return text
        head = limit // 2
        tail = limit - head
        return text[:head] + "\n...\n" + text[-tail:]
    
    def _get_tool_description(self) -> str:
        return (
            "Analyze multi-technology projects including Python, JavaScript, Docker, configs, etc. "
            "Uses universal text analysis for comprehensive project understanding. "
            "Input: a natural-language question about any aspect of the project."
        )


# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é RAG —Å–∏—Å—Ç–µ–º—É
RAGSystemFactory.register('universal', UniversalRAGSystem)
