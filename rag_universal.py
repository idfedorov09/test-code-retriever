#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª—é–±—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –≤–º–µ—Å—Ç–æ —Å–ª–æ–∂–Ω–æ–≥–æ AST –ø–∞—Ä—Å–∏–Ω–≥–∞.
"""

from __future__ import annotations

import os
import re
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS, DistanceStrategy
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

from rag_base import (
    BaseRAGSystem, BaseFileMap, BaseCodeElement, FileParser, DependencyAnalyzer,
    ProjectIndex, RAGSystemFactory, _relpath, _read_text
)


# -----------------------------------------------------------------------------
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
# -----------------------------------------------------------------------------

@dataclass
class UniversalCodeChunk(BaseCodeElement):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∫–æ–¥–∞"""
    content: str
    chunk_type: str  # "function", "class", "config", "generic"
    keywords: List[str] = field(default_factory=list)
    
    def to_line(self) -> str:
        return f"{self.chunk_type}: {self.name}  # L{self.lineno}-{self.end_lineno}"


@dataclass
class UniversalFileMap(BaseFileMap):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∫–∞—Ä—Ç–∞ —Ñ–∞–π–ª–∞"""
    chunks: List[UniversalCodeChunk]
    keywords: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    
    def to_text(self) -> str:
        parts = [f"FILE: {self.path} (type: {self.file_type})"]
        
        if self.keywords:
            parts.append("KEYWORDS: " + ", ".join(self.keywords[:10]))
        
        if self.dependencies:
            parts.append("DEPENDENCIES: " + ", ".join(self.dependencies[:5]))
        
        if self.chunks:
            parts.append("CHUNKS:")
            for chunk in self.chunks[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 —á–∞–Ω–∫–æ–≤
                parts.append(f"  - {chunk.to_line()}")
            if len(self.chunks) > 5:
                parts.append(f"  ... and {len(self.chunks) - 5} more chunks")
        
        parts.append(f"LOC: {self.loc}")
        return "\n".join(parts)
    
    def get_searchable_content(self) -> List[str]:
        content = []
        for chunk in self.chunks:
            content.append(f"{chunk.chunk_type}:{chunk.name}")
            content.extend([f"keyword:{kw}" for kw in chunk.keywords])
        return content


# -----------------------------------------------------------------------------
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
# -----------------------------------------------------------------------------

class UniversalFileParser:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤
    FILE_TYPE_PATTERNS = {
        '.py': 'python',
        '.js': 'javascript', '.ts': 'typescript', '.jsx': 'react', '.tsx': 'react',
        '.html': 'html', '.css': 'css', '.scss': 'sass',
        '.sql': 'sql',
        '.yml': 'yaml', '.yaml': 'yaml',
        '.json': 'json',
        '.md': 'markdown', '.rst': 'rst',
        '.txt': 'text', '.log': 'log',
        '.env': 'env',
        '.toml': 'toml', '.ini': 'ini',
        '.sh': 'shell', '.bash': 'shell',
    }
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
    CONTEXT_PROMPTS = {
        'python': "Python –∫–æ–¥: —Ñ—É–Ω–∫—Ü–∏–∏, –∫–ª–∞—Å—Å—ã, –∏–º–ø–æ—Ä—Ç—ã, PEP8 —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã",
        'javascript': "JavaScript/TypeScript: —Ñ—É–Ω–∫—Ü–∏–∏, –∫–ª–∞—Å—Å—ã, –º–æ–¥—É–ª–∏, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏",
        'react': "React –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: JSX, —Ö—É–∫–∏, —Å–æ—Å—Ç–æ—è–Ω–∏–µ, props",
        'html': "HTML —Ä–∞–∑–º–µ—Ç–∫–∞: —Å–µ–º–∞–Ω—Ç–∏–∫–∞, –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å, SEO",
        'css': "CSS —Å—Ç–∏–ª–∏: —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, flexbox, grid, –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å",
        'sql': "SQL –∑–∞–ø—Ä–æ—Å—ã: —Ç–∞–±–ª–∏—Ü—ã, –∏–Ω–¥–µ–∫—Å—ã, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å",
        'yaml': "YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
        'json': "JSON –¥–∞–Ω–Ω—ã–µ: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –≤–∞–ª–∏–¥–∞—Ü–∏—è",
        'dockerfile': "Docker: –æ–±—Ä–∞–∑—ã, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞",
        'markdown': "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —Å—Å—ã–ª–∫–∏, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
        'shell': "Shell —Å–∫—Ä–∏–ø—Ç—ã: –∫–æ–º–∞–Ω–¥—ã, –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
        'env': "–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: —Å–µ–∫—Ä–µ—Ç—ã, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
    }
    
    def can_parse(self, file_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª —Ç–µ–∫—Å—Ç–æ–≤—ã–º"""
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ –Ω–µ–Ω—É–∂–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        path_parts = Path(file_path).parts
        skip_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'venv', 'env', '.env', 'dist', 'build'}
        
        if any(part in skip_dirs for part in path_parts):
            return False
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                f.read(1024)  # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ø–µ—Ä–≤—ã–µ 1KB
            return True
        except:
            return False
    
    def parse(self, file_path: str, root: str) -> Optional[UniversalFileMap]:
        try:
            content = _read_text(file_path)
            file_type = self._detect_file_type(file_path)
            
            chunks = self._split_into_chunks(content, file_type)
            keywords = self._extract_keywords(content, file_type)
            dependencies = self._extract_dependencies(content, file_type)
            
            return UniversalFileMap(
                path=_relpath(file_path, root),
                file_type=file_type,
                chunks=chunks,
                keywords=keywords,
                dependencies=dependencies,
                loc=len(content.splitlines())
            )
        except Exception:
            return None
    
    def _detect_file_type(self, path: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ–∞–π–ª–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        ext = Path(path).suffix.lower()
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏
        filename = Path(path).name.lower()
        if 'dockerfile' in filename:
            return 'dockerfile'
        if filename in {'makefile', 'cmake', 'cmakelists.txt'}:
            return 'build'
        if filename.startswith('.'):
            return 'config'
        
        return self.FILE_TYPE_PATTERNS.get(ext, 'text')
    
    def _split_into_chunks(self, content: str, file_type: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–∞—Å—Ç–∏"""
        if file_type == 'python':
            return self._split_python_chunks(content)
        elif file_type in ['javascript', 'typescript', 'react']:
            return self._split_js_chunks(content)
        elif file_type == 'dockerfile':
            return self._split_dockerfile_chunks(content)
        elif file_type in ['yaml', 'json']:
            return self._split_config_chunks(content)
        elif file_type == 'sql':
            return self._split_sql_chunks(content)
        else:
            return self._split_generic_chunks(content)
    
    def _split_python_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ Python –∫–æ–¥–∞ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å—ã"""
        chunks = []
        lines = content.splitlines()
        current_chunk = []
        current_name = "module_level"
        current_type = "generic"
        start_line = 1
        
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            
            if stripped.startswith(('def ', 'class ', 'async def ')):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —á–∞–Ω–∫
                if current_chunk:
                    chunks.append(UniversalCodeChunk(
                        name=current_name,
                        content='\n'.join(current_chunk),
                        chunk_type=current_type,
                        lineno=start_line,
                        end_lineno=i-1,
                        keywords=self._extract_python_keywords('\n'.join(current_chunk))
                    ))
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫
                current_chunk = [line]
                start_line = i
                
                if stripped.startswith('class '):
                    current_type = "class"
                    current_name = stripped.split()[1].split('(')[0].rstrip(':')
                else:
                    current_type = "function"
                    current_name = stripped.split()[1].split('(')[0]
            else:
                current_chunk.append(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append(UniversalCodeChunk(
                name=current_name,
                content='\n'.join(current_chunk),
                chunk_type=current_type,
                lineno=start_line,
                end_lineno=len(lines),
                keywords=self._extract_python_keywords('\n'.join(current_chunk))
            ))
        
        return chunks
    
    def _split_js_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ JavaScript/React –∫–æ–¥–∞"""
        chunks = []
        lines = content.splitlines()
        current_chunk = []
        current_name = "module_level"
        current_type = "generic"
        start_line = 1
        
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            
            # –§—É–Ω–∫—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å—ã
            if (stripped.startswith(('function ', 'const ', 'let ', 'var ', 'class ', 'export function')) or
                'function' in stripped or '=>' in stripped):
                
                if current_chunk:
                    chunks.append(UniversalCodeChunk(
                        name=current_name,
                        content='\n'.join(current_chunk),
                        chunk_type=current_type,
                        lineno=start_line,
                        end_lineno=i-1,
                        keywords=self._extract_js_keywords('\n'.join(current_chunk))
                    ))
                
                current_chunk = [line]
                start_line = i
                
                if 'class ' in stripped:
                    current_type = "class"
                    current_name = re.search(r'class\s+(\w+)', stripped)
                    current_name = current_name.group(1) if current_name else "unknown_class"
                else:
                    current_type = "function"
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–º—è —Ñ—É–Ω–∫—Ü–∏–∏
                    match = re.search(r'(?:function\s+(\w+)|const\s+(\w+)|let\s+(\w+)|var\s+(\w+))', stripped)
                    current_name = next((g for g in match.groups() if g), "anonymous") if match else "anonymous"
            else:
                current_chunk.append(line)
        
        if current_chunk:
            chunks.append(UniversalCodeChunk(
                name=current_name,
                content='\n'.join(current_chunk),
                chunk_type=current_type,
                lineno=start_line,
                end_lineno=len(lines),
                keywords=self._extract_js_keywords('\n'.join(current_chunk))
            ))
        
        return chunks
    
    def _split_dockerfile_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ Dockerfile –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏"""
        chunks = []
        lines = content.splitlines()
        current_chunk = []
        current_name = "dockerfile_block"
        start_line = 1
        
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            
            if stripped.startswith(('FROM ', 'RUN ', 'COPY ', 'ADD ', 'WORKDIR ', 'EXPOSE ')):
                if current_chunk:
                    chunks.append(UniversalCodeChunk(
                        name=current_name,
                        content='\n'.join(current_chunk),
                        chunk_type="docker_command",
                        lineno=start_line,
                        end_lineno=i-1,
                        keywords=self._extract_docker_keywords('\n'.join(current_chunk))
                    ))
                
                current_chunk = [line]
                start_line = i
                current_name = stripped.split()[0].lower() + "_block"
            else:
                current_chunk.append(line)
        
        if current_chunk:
            chunks.append(UniversalCodeChunk(
                name=current_name,
                content='\n'.join(current_chunk),
                chunk_type="docker_command",
                lineno=start_line,
                end_lineno=len(lines),
                keywords=self._extract_docker_keywords('\n'.join(current_chunk))
            ))
        
        return chunks
    
    def _split_config_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        # –î–ª—è YAML/JSON —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π —á–∞–Ω–∫ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        return [UniversalCodeChunk(
            name="config",
            content=content,
            chunk_type="configuration",
            lineno=1,
            end_lineno=len(content.splitlines()),
            keywords=self._extract_config_keywords(content)
        )]
    
    def _split_sql_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ SQL —Ñ–∞–π–ª–æ–≤"""
        chunks = []
        statements = re.split(r';\s*\n', content)
        
        for i, stmt in enumerate(statements):
            if stmt.strip():
                stmt_type = "query"
                stmt_name = f"statement_{i+1}"
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø SQL –≤—ã—Ä–∞–∂–µ–Ω–∏—è
                first_word = stmt.strip().split()[0].upper() if stmt.strip() else ""
                if first_word in ['CREATE', 'ALTER', 'DROP']:
                    stmt_type = "ddl"
                elif first_word in ['SELECT', 'INSERT', 'UPDATE', 'DELETE']:
                    stmt_type = "dml"
                
                chunks.append(UniversalCodeChunk(
                    name=stmt_name,
                    content=stmt,
                    chunk_type=stmt_type,
                    lineno=1,  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ
                    end_lineno=len(stmt.splitlines()),
                    keywords=self._extract_sql_keywords(stmt)
                ))
        
        return chunks
    
    def _split_generic_chunks(self, content: str) -> List[UniversalCodeChunk]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã"""
        paragraphs = re.split(r'\n\s*\n', content)
        chunks = []
        
        for i, para in enumerate(paragraphs):
            if para.strip():
                chunks.append(UniversalCodeChunk(
                    name=f"section_{i+1}",
                    content=para,
                    chunk_type="text_section",
                    lineno=1,  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ
                    end_lineno=len(para.splitlines()),
                    keywords=self._extract_text_keywords(para)
                ))
        
        return chunks
    
    def _extract_keywords(self, content: str, file_type: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞"""
        if file_type == 'python':
            return self._extract_python_keywords(content)
        elif file_type in ['javascript', 'typescript', 'react']:
            return self._extract_js_keywords(content)
        elif file_type == 'dockerfile':
            return self._extract_docker_keywords(content)
        elif file_type in ['yaml', 'json']:
            return self._extract_config_keywords(content)
        elif file_type == 'sql':
            return self._extract_sql_keywords(content)
        else:
            return self._extract_text_keywords(content)
    
    def _extract_python_keywords(self, content: str) -> List[str]:
        keywords = []
        # –ò–º–ø–æ—Ä—Ç—ã
        keywords.extend(re.findall(r'import\s+(\w+)', content))
        keywords.extend(re.findall(r'from\s+(\w+)', content))
        # –§—É–Ω–∫—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å—ã
        keywords.extend(re.findall(r'def\s+(\w+)', content))
        keywords.extend(re.findall(r'class\s+(\w+)', content))
        return list(set(keywords))
    
    def _extract_js_keywords(self, content: str) -> List[str]:
        keywords = []
        # –ò–º–ø–æ—Ä—Ç—ã –∏ —ç–∫—Å–ø–æ—Ä—Ç—ã
        keywords.extend(re.findall(r'import.*from\s+[\'"]([^\'"]+)[\'"]', content))
        keywords.extend(re.findall(r'export\s+(?:function\s+)?(\w+)', content))
        # –§—É–Ω–∫—Ü–∏–∏
        keywords.extend(re.findall(r'function\s+(\w+)', content))
        keywords.extend(re.findall(r'const\s+(\w+)\s*=', content))
        return list(set(keywords))
    
    def _extract_docker_keywords(self, content: str) -> List[str]:
        keywords = []
        # Docker –∫–æ–º–∞–Ω–¥—ã
        keywords.extend(re.findall(r'FROM\s+([^\s]+)', content))
        keywords.extend(re.findall(r'COPY\s+([^\s]+)', content))
        keywords.extend(re.findall(r'RUN\s+(\w+)', content))
        return list(set(keywords))
    
    def _extract_config_keywords(self, content: str) -> List[str]:
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–π –∏–∑ YAML/JSON
        keywords = re.findall(r'(\w+):', content)
        return list(set(keywords))
    
    def _extract_sql_keywords(self, content: str) -> List[str]:
        keywords = []
        # –¢–∞–±–ª–∏—Ü—ã
        keywords.extend(re.findall(r'FROM\s+(\w+)', content, re.IGNORECASE))
        keywords.extend(re.findall(r'JOIN\s+(\w+)', content, re.IGNORECASE))
        keywords.extend(re.findall(r'TABLE\s+(\w+)', content, re.IGNORECASE))
        return list(set(keywords))
    
    def _extract_text_keywords(self, content: str) -> List[str]:
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
        words = re.findall(r'\b[A-Za-z_]\w+\b', content)
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –æ–±—â–∏–µ —Å–ª–æ–≤–∞
        keywords = [w for w in words if len(w) > 3 and w.lower() not in {'this', 'that', 'with', 'from', 'have', 'will', 'been', 'were'}]
        return list(set(keywords[:20]))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def _extract_dependencies(self, content: str, file_type: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        deps = []
        
        if file_type == 'python':
            # Python –∏–º–ø–æ—Ä—Ç—ã
            deps.extend(re.findall(r'import\s+([^\s,]+)', content))
            deps.extend(re.findall(r'from\s+([^\s]+)\s+import', content))
        elif file_type in ['javascript', 'typescript']:
            # JS/TS –∏–º–ø–æ—Ä—Ç—ã
            deps.extend(re.findall(r'import.*from\s+[\'"]([^\'"]+)[\'"]', content))
            deps.extend(re.findall(r'require\([\'"]([^\'"]+)[\'"]\)', content))
        elif file_type == 'dockerfile':
            # Docker –æ–±—Ä–∞–∑—ã
            deps.extend(re.findall(r'FROM\s+([^\s]+)', content))
        
        return list(set(deps))


# -----------------------------------------------------------------------------
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞
# -----------------------------------------------------------------------------

class UniversalRAGSystem(BaseRAGSystem):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª—é–±—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self, llm, embeddings, **kwargs):
        parsers = [UniversalFileParser()]
        analyzers = []  # –ü–æ–∫–∞ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
        
        # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –ø–∞–º—è—Ç—å—é
        default_config = {
            'max_documents': 500,          # –ú–∞–∫—Å–∏–º—É–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            'max_chunks_per_file': 5,      # –ú–∞–∫—Å–∏–º—É–º —á–∞–Ω–∫–æ–≤ –Ω–∞ —Ñ–∞–π–ª
            'max_file_size_kb': 50,        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ KB
            'faiss_batch_size': 25,        # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è FAISS
            'bm25_k': 8,                   # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è BM25
            'dense_k': 8,                  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è dense retrieval
            'ensemble_weights': (0.6, 0.4), # BM25 –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ
            'map_char_budget': 16000,      # –ë—é–¥–∂–µ—Ç —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            'answer_language': 'ru'
        }
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        merged_config = {**default_config, **kwargs}
        
        super().__init__(llm, embeddings, parsers, analyzers, **merged_config)
        
        print(f"‚öôÔ∏è  Universal RAG –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
        print(f"   üìÑ –ú–∞–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {self.config.get('max_documents')}")
        print(f"   üì¶ –†–∞–∑–º–µ—Ä FAISS –±–∞—Ç—á–∞: {self.config.get('faiss_batch_size')}")
        print(f"   üîç –ß–∞–Ω–∫–æ–≤ –Ω–∞ —Ñ–∞–π–ª: {self.config.get('max_chunks_per_file')}")
    
    def get_file_patterns(self) -> Dict[str, str]:
        return {"all": "**/*"}  # –í—Å–µ —Ñ–∞–π–ª—ã
    
    def get_evidence_prompt_template(self) -> str:
        return """
You are a code reviewer analyzing a multi-language project. You will see a compact map of the repository with different file types and their contents.

The map shows:
- Files of different types (Python, JavaScript, Docker, YAML, etc.)
- Code chunks and their keywords
- Dependencies and relationships
- Configuration files and their settings

Given the user question, propose up to {max_items} precise evidence items (files/chunks to inspect) in JSON.
Each item must be an object with keys: file (relative path), symbol (chunk name or "*" for whole file), and reason.

IMPORTANT: 
- Paths must be repository-relative (e.g., `src/app.py`) ‚Äî NEVER absolute
- Copy the exact path shown after `FILE:` in the context
- Consider the file type when determining relevance
- For configuration questions, focus on config files (YAML, JSON, ENV)
- For architecture questions, look at multiple file types
- Include both specific chunks and whole files for complete analysis

Respond ONLY with JSON array, no prose.
"""
    
    def get_answer_prompt_template(self) -> str:
        return """
You are a senior software architect analyzing a multi-technology project for code review. Using the provided repository map and evidence, answer the question comprehensively.

The repository includes files of different types:
- Source code (Python, JavaScript, etc.)
- Configuration files (YAML, JSON, ENV)
- Infrastructure files (Dockerfile, docker-compose)
- Documentation (Markdown, README)
- Build files (Makefile, package.json)

For questions about:
- Architecture: Consider relationships between different components and technologies
- Configuration: Focus on settings, environment variables, and deployment setup
- Dependencies: Look at imports, package files, and external services
- Security: Check for hardcoded secrets, permissions, and best practices
- Performance: Analyze code patterns and configuration optimizations

Provide specific file:line references from the evidence and explain relationships clearly.
Be precise, cite concrete references as `file:line`, and provide actionable insights. 
Consider the multi-technology nature of the project in your analysis.
Reply in {answer_language}.
"""
    
    def _create_documents(self, file_maps: List[BaseFileMap]) -> List[Document]:
        docs = []
        universal_maps = [fm for fm in file_maps if isinstance(fm, UniversalFileMap)]
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –ø–∞–º—è—Ç—å—é
        MAX_DOCS = self.config.get('max_documents', 1000)
        MAX_CHUNKS_PER_FILE = self.config.get('max_chunks_per_file', 10)
        MAX_FILE_SIZE = self.config.get('max_file_size_kb', 100)  # KB
        
        print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(universal_maps)} —Ñ–∞–π–ª–æ–≤")
        print(f"‚öôÔ∏è  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: –º–∞–∫—Å {MAX_DOCS} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {MAX_CHUNKS_PER_FILE} —á–∞–Ω–∫–æ–≤/—Ñ–∞–π–ª")
        
        doc_count = 0
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ —Ä–∞–∑–º–µ—Ä—É (—Å–Ω–∞—á–∞–ª–∞ –º–∞–ª–µ–Ω—å–∫–∏–µ)
        universal_maps.sort(key=lambda fm: fm.loc)
        
        # Create documents from file maps
        for fm in universal_maps:
            if doc_count >= MAX_DOCS:
                print(f"‚ö†Ô∏è  –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({MAX_DOCS}), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã")
                break
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã
            file_size_kb = fm.loc * 0.05  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
            if file_size_kb > MAX_FILE_SIZE:
                print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª: {fm.path} (~{file_size_kb:.1f}KB)")
                continue
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –≤—Å–µ–≥–æ —Ñ–∞–π–ª–∞ (—Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)
            if fm.loc < 200:  # –¢–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã –º–µ–Ω—å—à–µ 200 —Å—Ç—Ä–æ–∫
                file_content = fm.to_text()
                docs.append(Document(
                    page_content=file_content,
                    metadata={"source": fm.path, "type": f"{fm.file_type}-map", "loc": fm.loc}
                ))
                doc_count += 1
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)
            context_prompt = UniversalFileParser.CONTEXT_PROMPTS.get(fm.file_type, "")
            
            # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ —á–∞–Ω–∫–∏
            important_chunks = self._select_important_chunks(fm.chunks, MAX_CHUNKS_PER_FILE)
            
            for chunk in important_chunks:
                if doc_count >= MAX_DOCS:
                    break
                    
                chunk_content = f"""FILE: {fm.path} (type: {fm.file_type})
CONTEXT: {context_prompt}
CHUNK: {chunk.name} ({chunk.chunk_type})

{chunk.content}

KEYWORDS: {', '.join(chunk.keywords[:5])}"""  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                
                docs.append(Document(
                    page_content=chunk_content,
                    metadata={
                        "source": fm.path,
                        "type": f"{fm.file_type}-chunk",
                        "chunk_name": chunk.name,
                        "chunk_type": chunk.chunk_type
                    }
                ))
                doc_count += 1
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤
        file_types_summary = ["PROJECT FILE TYPES SUMMARY:"]
        type_counts = {}
        for fm in universal_maps:
            type_counts[fm.file_type] = type_counts.get(fm.file_type, 0) + 1
        
        for file_type, count in sorted(type_counts.items()):
            context = UniversalFileParser.CONTEXT_PROMPTS.get(file_type, "")
            file_types_summary.append(f"  {file_type}: {count} files - {context}")
        
        docs.append(Document(
            page_content="\n".join(file_types_summary),
            metadata={"source": "__file_types__", "type": "project-summary"}
        ))
        doc_count += 1
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {doc_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∏–∑ {len(universal_maps)} —Ñ–∞–π–ª–æ–≤)")
        return docs
    
    def _select_important_chunks(self, chunks: List[UniversalCodeChunk], max_chunks: int) -> List[UniversalCodeChunk]:
        """–í—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        if len(chunks) <= max_chunks:
            return chunks
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —Ç–∏–ø—É —á–∞–Ω–∫–∞
        priority_order = {
            'class': 3,
            'function': 2,
            'configuration': 2,
            'docker_command': 1,
            'generic': 0,
            'text_section': 0
        }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ —Ä–∞–∑–º–µ—Ä—É
        sorted_chunks = sorted(
            chunks,
            key=lambda c: (
                priority_order.get(c.chunk_type, 0),
                len(c.keywords),  # –ë–æ–ª—å—à–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ = –≤–∞–∂–Ω–µ–µ
                len(c.content)    # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ = –≤–∞–∂–Ω–µ–µ
            ),
            reverse=True
        )
        
        return sorted_chunks[:max_chunks]
    
    def _build_search_index(
        self, 
        docs: List[Document], 
        project_path: str, 
        file_maps: List[BaseFileMap]
    ) -> ProjectIndex:
        print(f"üèóÔ∏è  –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º GPU —É—Ç–∏–ª–∏—Ç—ã –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        try:
            from gpu_utils import cleanup_gpu, monitor_gpu, gpu_manager
            GPU_UTILS_AVAILABLE = True
        except ImportError:
            GPU_UTILS_AVAILABLE = False
        
        # BM25 retriever (–±—ã—Å—Ç—Ä—ã–π, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU)
        print("üìù –°–æ–∑–¥–∞–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞...")
        bm25 = BM25Retriever.from_documents(docs)
        bm25.k = self.config.get('bm25_k', 12)
        
        # FAISS retriever —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        print("üîç –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º FAISS
        if GPU_UTILS_AVAILABLE:
            if gpu_manager.check_memory_threshold(70):
                print("‚ö†Ô∏è  –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏, –≤—ã–ø–æ–ª–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É...")
                cleanup_gpu()
            monitor_gpu()
        
        try:
            # –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            batch_size = self.config.get('faiss_batch_size', 50)  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä
            
            if len(docs) > batch_size:
                print(f"üì¶ –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ {batch_size}")
                
                # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å –ø–æ —á–∞—Å—Ç—è–º
                first_batch = docs[:batch_size]
                faiss = FAISS.from_documents(
                    first_batch, 
                    self.embeddings, 
                    distance_strategy=DistanceStrategy.COSINE
                )
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –±–∞—Ç—á–∞–º–∏
                remaining_docs = docs[batch_size:]
                for i in range(0, len(remaining_docs), batch_size):
                    batch = remaining_docs[i:i + batch_size]
                    batch_num = i // batch_size + 2
                    total_batches = (len(docs) - 1) // batch_size + 1
                    print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_num}/{total_batches} ({len(batch)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
                    
                    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                    if GPU_UTILS_AVAILABLE:
                        cleanup_gpu()
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –∏–Ω–¥–µ–∫—Å—É
                    batch_texts = [doc.page_content for doc in batch]
                    batch_metadatas = [doc.metadata for doc in batch]
                    faiss.add_texts(batch_texts, batch_metadatas)
            else:
                # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤
                print(f"üìÑ –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                faiss = FAISS.from_documents(
                    docs, 
                    self.embeddings, 
                    distance_strategy=DistanceStrategy.COSINE
                )
            
            print("‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞: {e}")
            if GPU_UTILS_AVAILABLE:
                cleanup_gpu()
            
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ BM25
            print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ BM25-only —Ä–µ–∂–∏–º")
            return ProjectIndex(
                root=project_path,
                docs=docs,
                retriever=bm25,
                file_maps=file_maps
            )
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞
        if GPU_UTILS_AVAILABLE:
            cleanup_gpu()
            print("üìä –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏:")
            monitor_gpu()
        
        dense_retriever = faiss.as_retriever(search_kwargs={"k": self.config.get('dense_k', 12)})
        
        # Ensemble retriever
        print("ü§ù –°–æ–∑–¥–∞–Ω–∏–µ ensemble retriever...")
        ensemble = EnsembleRetriever(
            retrievers=[bm25, dense_retriever], 
            weights=list(self.config.get('ensemble_weights', (0.5, 0.5)))
        )
        
        print("‚úÖ –ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤!")
        return ProjectIndex(
            root=project_path,
            docs=docs,
            retriever=ensemble,
            file_maps=file_maps
        )
    
    def _answer_question(self, question: str, index: ProjectIndex) -> str:
        # Retrieve relevant documents
        retrieved = index.retriever.invoke(question)
        
        # Gather context (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        pieces = []
        for d in retrieved[:10]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            pieces.append(f"---\n{d.page_content}\n")
        
        map_text = "\n".join(pieces)[:self.config.get('map_char_budget', 24000)]
        
        # Generate answer directly (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ evidence planning)
        answer_prompt = ChatPromptTemplate.from_template(self.get_answer_prompt_template())
        answer_chain = answer_prompt | self.llm | StrOutputParser()
        
        final = answer_chain.invoke({
            "question": question,
            "map_text": map_text,
            "answer_language": self.config.get('answer_language', 'ru'),
            "evidence_text": "(using retrieved context)",
        })
        
        return final
    
    def _get_tool_description(self) -> str:
        return (
            "Analyze multi-technology projects including Python, JavaScript, Docker, configs, etc. "
            "Uses universal text analysis for comprehensive project understanding. "
            "Input: a natural-language question about any aspect of the project."
        )


# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é RAG —Å–∏—Å—Ç–µ–º—É
RAGSystemFactory.register('universal', UniversalRAGSystem)
